---
title : "CrystalFormer-RL: Reinforcement Fine-Tuning for Crystal Design"
date : 2025-04-19
categories : 
- DeePMD
---

Recently, the Institute of Physics, Chinese Academy of Sciences, released the crystal generation model CrystalFormer-RL based on reinforcement learning fine-tuning. It further enhances the directional generation ability of the crystal material generation model by fine-tuning through reinforcement learning on the basis of the existing crystal generation model CrystalFormer.

<!-- more -->

## Project Background

Traditional material research and development is often costly, time-consuming, and inefficient. With the rapid development of artificial intelligence (AI), materials science has ushered in new opportunities. Currently, in the field of crystal materials, AI is mainly applied to two categories of tasks:

- **Discriminative models**: The goal of these models is to "understand" materials. For example, machine - learning force fields such as the DPA series, MACE, and Orb can accurately simulate inter - atomic interactions and are used for simulation calculations of large - scale systems. Property prediction models like CGCNN and MEGNet can predict key physical properties of materials, such as formation energy, bandgap, and dielectric constant, helping us quickly screen out "worthy - of - attention" materials.

- **Generative models**: These models are more like "creators". Representative ones include CDVAE, DiffCSP, MatterGen, CrystalFormer, etc. They learn the laws of material structures from existing crystal structure databases and then "generate" new and potentially synthesizable crystal structures.

## Project Introduction

First, as the foundation for reinforcement learning, CrystalFormer is a Transformer-based autoregressive model that learns the "language of crystals" from crystal material databases. Its design leverages natureâ€™s embedded codebook: the Wyckoff position tables of space groups. CrystalFormer-RL introduces a reinforcement learning training strategy on this basis, optimizing generated results through reward signals from discriminative models (machine learning force fields or property prediction models), such as "lower energy is better" or "both larger bandgap and dielectric constant are better."The following is a flow chart of reinforcement learning.

<center>
<img src="https://dp-public.oss-cn-beijing.aliyuncs.com/community/Blog%20Files/DeePMD_18_04_2025/p1.webp">
</center>

*Figure: Flowchart of reinforcement learning fine-tuning*

ðŸ§ª Two Core Application Examples:

**1.Enhancing Crystal Structure Stability:** The total energy of generated crystals is predicted using a machine learning force field, and the energy above the convex hull (energy above the convex hull) is calculated as the reward function. After reinforcement learning fine-tuning, the proportion of stable crystals generated significantly increasedâ€”from the original 44.7% to 73.4%. Fine-tuning also increases the proportion of stable, unique, and novel (S.U.N) samples generated.

<center>
<img src="https://dp-public.oss-cn-beijing.aliyuncs.com/community/Blog%20Files/DeePMD_18_04_2025/p2.webp">
</center>

*Energy distribution histogram of crystal samples relative to the convex hull predicted by the Orb model, corresponding to the pre-trained model and the reinforcement learning fine-tuned model*

<center>
<img src="https://dp-public.oss-cn-beijing.aliyuncs.com/community/Blog%20Files/DeePMD_18_04_2025/p3.webp">
</center>

*In the 14 space groups encompassed by the seven crystal systems, the proportions of the resulting structures that conform to the S.U.N. (stable: Ehull < 0.1 eV/atom, unique, novel) criterion are obtained.*

**2.Discovering High-Performance Dielectric Materials:** In dielectric materials, bandgap and dielectric constant often exhibit an "inverse" relationshipâ€”large bandgap typically implies low dielectric constant, and vice versa. To balance these two properties, the figure of merit (FoM), defined as FoM = bandgap Ã— dielectric constant, is used as the reward function to guide model optimization. Through reinforcement learning training, the model gradually breaks through the original Pareto frontier between bandgap and dielectric constant, successfully generating a series of dielectric materials with higher FoM and reasonable structures.

<center>
<img src="https://dp-public.oss-cn-beijing.aliyuncs.com/community/Blog%20Files/DeePMD_18_04_2025/p4.webp">
</center>

For more results and implementation details, refer to the paper: https://arxiv.org/abs/2504.02367.

The CrystalFormer-RL project has been open-sourced under the Apache-2.0 license in the DeepModeling community:
https://github.com/deepmodeling/CrystalFormer

The project includes models trained on the MP20 dataset (2) and Alex20 (3). For any issues during use, feel free to raise Issues (4) or Discussions (5) in the CrystalFormer GitHub project.

## Case Demonstration

To facilitate quick adoption of CrystalFormer-RL, an introductory tutorial is available on the Bohrium platform.

Notebook Link: https://j1q.cn/zxU5idus

## Future Outlook

**CrystalFormer-RL** demonstrates a flexible and universal framework: it seamlessly connects existing generative models with discriminative models through reinforcement learning, cleverly combining their advantages. This not only improves material design efficiency but also opens an exciting door for the collaborative development of the materials machine learning ecosystem.
Future prospects for **CrystalFormer-RL include**:

**1.Screening and Optimization of Multiphasic Materials:** Such as multi-objective optimization in complex systems like thermoelectrics and superconductors;

**2.Flexible Customization of Reward Mechanisms:** One of RLâ€™s greatest advantages is flexibilityâ€”reward functions can be customized according to specific needs, or different discriminative models can be switched to achieve results more aligned with design goals;

**3.Generative Models Feeding Back into Discriminative Models:** Currently, CrystalFormer-RL primarily demonstrates the guidance of discriminative models on generative models. In the future, generative models are expected to provide richer training data and structural samples for discriminative models, achieving bidirectional improvement.

**References**

[1] Z. Cao and L. Wang, CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design, arXiv:2504.02367.

[2] https://github.com/txie-93/cdvae/tree/main/data/mp_20

[3] https://huggingface.co/datasets/zdcao/alex-20

[4] https://github.com/deepmodeling/CrystalFormer/issues

[5] https://github.com/deepmodeling/CrystalFormer/discussions
